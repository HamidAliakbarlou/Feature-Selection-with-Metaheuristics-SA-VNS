# -*- coding: utf-8 -*-
"""Final_German_SA_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fyDzRVtw7dXZdQ3u1toGLXR5WFaP91hu

# Import dataset and library
"""

import pandas as pd
import numpy as np
import random
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing

#Read & Prepare data

#Load data
Xtrain = pd.read_csv('/content/Xtrain.csv',header=0)
ytrain = pd.read_csv('/content/ytrain.csv',header=0)
Xvalid = pd.read_csv('/content/Xvalid.csv',header=0)
yvalid = pd.read_csv('/content/yvalid.csv',header=0)
Xtest = pd.read_csv('/content/Xtest.csv',header=0)
ytest = pd.read_csv('/content/ytest.csv',header=0)
ytrain, yvalid, ytest =  ytrain["GoodCredit"], yvalid["GoodCredit"], ytest["GoodCredit"]

#Scaling features to a range
min_max_scaler = preprocessing.MinMaxScaler()
Xtrain = pd.DataFrame(min_max_scaler.fit_transform(Xtrain))
Xvalid = pd.DataFrame(min_max_scaler.fit_transform(Xvalid))
Xtest = pd.DataFrame(min_max_scaler.fit_transform(Xtest))

column_names = ['AccountStatus', 'Duration', 'CreditHistory', 'Purpose', 'CreditAmount', 'Account',
 'EmploymentSince', 'PrecentOfIncome', 'PersonalStatus', 'OtherDebtors', 'ResidenceSince',
 'Property', 'Age', 'OtherInstallPlans', 'Housing', 'NumExistingCredits', 'Job',
 'NumMaintenance', 'Telephone', 'ForeignWorker', 'sexe']

Xtrain = Xtrain.set_axis(column_names, axis=1, inplace=False)
Xvalid = Xvalid.set_axis(column_names, axis=1, inplace=False)
Xtest = Xtest.set_axis(column_names, axis=1, inplace=False)

"""

# Evaluation function"""

def Metric_Acc(new_subset):
    """
    Run Logisticregression classification model on feature subset
    and retrieve the accuracy score
    """
    Xtrain_temp = Xtrain.iloc[:, list(new_subset)]
    Xvalid_temp = Xvalid.iloc[:, list(new_subset)]
    LR = LogisticRegression(solver="liblinear")  
#-----------------------------------
    from sklearn.svm import SVC
    LR =  SVC(gamma='auto')
#------------------------------------
    # fit and score
    LR.fit(Xtrain_temp, ytrain)
    return LR.score(Xvalid_temp, yvalid) # Accuracy

"""# Acc without implementing SA"""

from sklearn.svm import SVC
LR =  SVC(gamma='auto')

# Finding initial  Valid set & Test set accuracy with fullset features
full_set = set(np.arange(len(Xtrain.columns)))

Xtrain_temp = Xtrain.iloc[:, list(full_set)]
Xtest_temp = Xtest.iloc[:, list(full_set)]
Xvalid_temp = Xvalid.iloc[:, list(full_set)]

# fit and score
LR.fit(Xtrain_temp, ytrain)

# Accuracy score
valid_Acc_Score=LR.score(Xvalid_temp, yvalid)
print("Without SA: Acc for valid_set:", valid_Acc_Score)
#------------------------------
Test_Acc_Score=LR.score(Xtest_temp, ytest)
print("\n Without SA: Acc for test_set:", Test_Acc_Score, '\n')

"""# Random_features_set function

"""

def random_features_set(curr_subset,
                        hash_values):
        """
        "Execute pertubation" (i.e. alter current subset to get new subset)
        ### Break: now the new subset is made, let's go for evaluation 
        """
        full_set = set(np.arange(len(Xtrain.columns)))

        while True:
            # Decide what type of pertubation to make
            if len(curr_subset) == len(full_set): 
                move = 'Remove'
            elif len(curr_subset) == 2: # Not to go below 2 features
                move = random.choice(['Add', 'Replace'])
            else:
                move = random.choice(['Add', 'Replace', 'Remove'])
            
            # Get columns not yet used in current subset
            pending_cols = full_set.difference(curr_subset) 
            new_subset = curr_subset.copy()   

            if move == 'Add':        
                new_subset.add(random.choice(list(pending_cols)))
            elif move == 'Replace': 
                new_subset.remove(random.choice(list(curr_subset)))
                new_subset.add(random.choice(list(pending_cols)))
            else:
                new_subset.remove(random.choice(list(curr_subset)))
                
            if new_subset in hash_values:
                #print('Subset already visited')
                continue
            else:
                hash_values.add(frozenset(new_subset))
                break

        return new_subset, hash_values

"""# Local Search function"""

def LocalSearch(results, i,
                metric, prev_metric, best_metric,
                new_subset, best_subset, curr_subset,
                beta, T):
        """
       LocalSearch: Returns set of index of the features to be included
        """                                                      
        if metric > prev_metric:
            #---------print('Local improvement in metric from {:8.4f} to {:8.4f} '
                  #------------.format(prev_metric, metric) + ' - New subset accepted')
            outcome = 'Improved'
            accept_prob, rnd = '-', '-'
            prev_metric = metric
            curr_subset = new_subset.copy()

            # Keep track of overall best metric so far
            if metric > best_metric:
                print(f'Starting Iteration {i+1}')
                print('Global improvement in metric from {:8.4f} to {:8.4f} '
                      .format(best_metric, metric) + ' - Best subset updated')
                best_metric = metric
                best_subset = new_subset.copy()

        # "Probability": If the new metric is worse, Let's Find the probability to accept or reject a worse solution        
        else:
            rnd = np.random.uniform()
            diff = prev_metric - metric
            accept_prob = np.exp(-beta * diff / T)

            # Accept the worse solution.
            if rnd < accept_prob:

                outcome = 'Accept'
                prev_metric = metric
                curr_subset = new_subset.copy()

            # Reject the worse solution 
            else:
                outcome = 'Reject'

        # Update results dataframe
        results.loc[i, 'Iteration'] = i+1
        results.loc[i, 'Feature Count'] = len(curr_subset)
        results.loc[i, 'Feature Set'] = sorted(curr_subset)
        results.loc[i, 'Metric'] = metric
        results.loc[i, 'Best Metric'] = best_metric
        results.loc[i, 'Acceptance Probability'] = accept_prob
        results.loc[i, 'Random Number'] = rnd
        results.loc[i, 'Outcome'] = outcome

        return results, metric, prev_metric, best_metric, curr_subset, best_subset

"""# Simulated Annealing (main) function

---



"""

# Setup simulated annealing algorithm
def simulated_annealing(maxiters,
                        alpha,
                        beta):
  
    """
    Function to perform feature selection using simulated annealing
    Inputs:
    #X: Predictor features
    #y: target labels
    maxiters: Maximum number of iterations
    alpha: Factor to reduce temperature
    beta: Constant in probability estimate 
    T_0: Initial temperature
    ## temp_reduction: Strategy for temperature reduction schedule
    Output:
    1) Dataframe of parameters explored and corresponding model performance
    2) Best metric score (i.e. AUC  score in this case)
    3) List of subset features that correspond to the best metric
    """
    temp_reduction = 'geometric'
    T_0=1

    Outputcolumns = ['Iteration', 'Feature Count', 'Feature Set', 
               'Metric', 'Best Metric', 'Acceptance Probability', 
               'Random Number', 'Outcome']

    results = pd.DataFrame(index=range(maxiters), columns=Outputcolumns)
    best_subset = None
    hash_values = set()
    
    T = T_0

    # Get ascending range indices of all columns    
    full_set = set(np.arange(len(Xtrain.columns)))

    # Generate initial random subset based on ~50% of columns
    curr_subset = full_set

    # Get baseline metric score (i.e. AUC) of initial random subset
    prev_metric = Metric_Acc(curr_subset)
    best_metric = prev_metric


    # "Main Iteration loop": 1-Find new subset(solution)  2-Evalute  3-Report the i#th iteration result 
    for i in range(maxiters):

        # "Execute pertubation" (i.e. alter current subset to get new subset)
        new_subset, hash_values= random_features_set(curr_subset,
                                                     hash_values)

        # "Evaluation": Get metric of new subset
        metric = Metric_Acc(new_subset)

        # "LocalSearch": Returns set of index of the features to be included
        results, metric, prev_metric, best_metric, curr_subset, best_subset = LocalSearch(results, i,
                                                                                          metric, prev_metric,best_metric,
                                                                                          new_subset,best_subset,curr_subset,
                                                                                          beta, T)
        # Temperature cooling schedule
        if temp_reduction == 'geometric':
            T = alpha * T
        elif temp_reduction == 'linear':
            T -= alpha
        elif temp_reduction == 'slow decrease':
            b = 5 # Arbitrary constant
            T = T / (1 + b * T)
            print('T',T)
        else:
            raise Exception("Temperature reduction strategy not recognized")

                
    ##  "END of iterations": Now, let's make the final results

    # Drop NaN rows in results
    results = results.dropna(axis=0, how='all')

    # Save results as CSV
    results.to_csv('/sa_output.csv', index=False)

    # Finding Final Test set accuracy
    Xtrain_temp = Xtrain.iloc[:, list(best_subset)]
    Xtest_temp = Xtest.iloc[:, list(best_subset)]
    LR = LogisticRegression(solver="liblinear") 
#--------------------------------
    from sklearn.svm import SVC
    LR =  SVC(gamma='auto')
#--------------------------------
    # fit and score
    LR.fit(Xtrain_temp, ytrain)
    Test_Acc_Score=LR.score(Xtest_temp, ytest)
    print('Test Accuracy Score', Test_Acc_Score)
    print('Best_Subset \n', best_subset)


    return best_metric, Test_Acc_Score

def Multiple_RUN(iteration):

    output_valid = pd.DataFrame(index=range(iteration), columns=['Best_metrics'])
    output_test = pd.DataFrame(index=range(iteration), columns=['Best_metrics'])

    for j in range(iteration):
       BEST_METRIC, Test_Acc_Score= simulated_annealing(500,0.98,1)
       print("============================")
       output_valid.loc[j] = BEST_METRIC
       output_test.loc[j] = Test_Acc_Score

       print('Best metric({}th) is {:8.4f}\n \n \n \n \n'.format(j+1, BEST_METRIC))
    print("============== end ==============")

    print('Output of Best metrics includes on valid set: \n', output_valid)
    Ave_BestMetrics = output_valid["Best_metrics"].mean()
    print('\n Average of Best metrics is: \n',  Ave_BestMetrics)

    print('Output of Best metrics on test set includes: \n', output_test)
    Ave_BestMetrics_test = output_test["Best_metrics"].mean()
    print('\n Average of Best metrics on test set is: \n',  Ave_BestMetrics_test)


Multiple_RUN(10)